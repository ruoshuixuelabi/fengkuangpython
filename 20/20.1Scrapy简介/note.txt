在学习第19章数据可视化时,读者可能发现一个问题：数据从何而来?从第19章的最后一个示例来看,可视化分析的数据很有可能来自网络。从某种意义上看,网络才是最大的数据库,互联 网上的每个用户既是这些数据的创作者,也是这些数据的使用者。

有一种技术能自动从网上下载并提取项目感兴趣的海量数据,这种技术就是爬虫。实际上,爬虫项目也是 Python 最热门的应用之一 。
从广义上看,使用 Python 开发爬虫项目的方法有很多,即使只是简单地使用 Python 自带的 urllib 和 re 也可以实现爬虫项目,
但这种方式太原始了,本章的项目不打算使用这种方式。(既然有跑车开,为何还要步行去远方?)本章打算介绍一个专业的爬虫开发框架：
Scrapy,Scrapy 本身已经整合了大量工具包(比如Twisted、Ixml、cssselect等 ),因此使用Scrapy 框架开发爬虫项目非常简单、方便。

下面就从安装Scrapy开始介绍。

20.1	Scrapy简介

与其他 Python 包一样,Scrapy 也不是 Python 内置的包,因此必须先安装Scrapy,然后才能使用它。

20.1.1 了解Scrapy

第19章在介绍数据可视化分析时,最后一个示例示范了展示从网络上获取的天气信息。从广义上说,那个例子也属于网络爬虫：
程序可以自动获取多个页面中的所有天气信息。如果使用某种技术(如正则表达式、XPath 等)来提取页面中所有的链接(<a…/>元素),
然后顺着这些链接递归 打开对应的页面,最后提取页面中的信息,这就是网络爬虫。

既然第19章已经介绍了网络爬虫的知识,那么本章还有存在的意义吗?在回答这个问题之前,我们来分析网络爬虫具体要做哪些核心工作?
①通过网络向指定的 URL 发送请求,获取服务器响应内容。
②使用某种技术(如正则表达式、XPath 等)提取页面中我们感兴趣的信息。
③高效地识别响应页面中的链接信息,顺着这些链接递归执行此处介绍的①②③步。
④使用多线程有效地管理网络通信交互。

如果直接使用 Python 内置的 urllib 和 re 模块是否能写出自己的网络爬虫呢?答案是肯定的,只是比较复杂。
就像我们要从广州去韶关,走路可以去吗?答案是肯定的,只是比较麻烦。

下面继续分析网络爬虫的核心工作。
(1)向 URL 发送请求,获取服务器响应内容。这个核心工作其实是所有网络爬虫都需要做的通用工作。
一般来说,通用工作应该由爬虫框架来实现,这样可以提供更稳定的性能,开发效率更高。
(2)提取页面中我们感兴趣的信息。这个核心工作不是通用的!每个项目感兴趣的信息都可能有所不同,
但使用正则表达式提取信息是非常低效的,原因是正则表达式的设计初衷主要是处理文本信息,而 HTML 文档不仅是文本文档,
而且是结构化文档,因此使用正则表达式来处理 HTML 文档并不合适。使用 XPath 提取信息的效率要高得多。
(3)识别响应页面中的链接信息。使用正则表达式可以实现这个核心工作,但是效率太低,使用 XPath 会更高效。
(4)多线程管理：这个核心工作是通用的,应该由框架来完成。

提示：关于XPath 的内容,如果读者手上有《疯狂XML讲义》,则可以查阅该书第9章;
如果不懂 XPath 的相关知识,后面会简单地介绍本章项目所涉及的内容

现在来回答上面提出的问题：本章有存在的意义吗?当然有,本章并不介绍使用 urllib、re模块这种简陋的工具来实现正则表达式,
而是通过专业的爬虫框架Scrapy 来实现爬虫。

Scrapy是一个专业的、高效的爬虫框架,它使用专业的Twisted包(基于事件驱动的网络引擎包)高效地处理网络通信,
使用lxml(专业的XML处理包)、cssselect高效地提取HTML 页面的有效信息,同时它也提供了有效的线程管理。

一言以蔽之,上面列出的网络爬虫的核心工作,Scrapy 全部提供了实现,开发者只要使用 XPath 或 CSS 选择器定义自己感兴趣的信息即可。

20.1.2	安装Scrapy

安装 Scrapy 与安装其他 Python 包没有区别,同样使用如下命令来安装。
pip  install   scrapy

如果在命令行窗口中运行该命令,将会看到程序并不立即下载、安装 Scrapy, 而是不断地下载大量第三方包。
提示：如果在命令行窗口中提示找不到pip命令,则也可以通过 python 命令运行 pip 模块。例如 python -m pip install scrapy。

这是因为Scrapy 需要依赖大量第三方包。典型的,Scrapy 需要依赖如下第三方包。
(1)pyOpenSSL：Python用于支持SSL(Security Socket Layer)的包。
(2)cryptography：Python 用于加密的库。
(3)CFFI：Python 用于调用C 的接口库。
(4)zope.interface：为 Python 缺少接口而提供扩展的库。
(5)Ixml：一个处理XML、HTML 文档的库,比Python 内置的xml 模块更好用。
(6)cssselect：Python 用于处理CSS 选择器的扩展包。
(7)Twisted：为 Python 提供的基于事件驱动的网络引擎包。

如果在Python 环境下没有这些第三方包,那么Python 会根据依赖自动下载并安装它们。这个过程原本没啥好讲的,
pip 通常会自动完成整个过程,我们只需要等待即可。

但pip 在自动下载、安装Twisted时会提示以下错误。
error:Microsoft Visual C++ 14.0  is required. Get  it  with "Microsoft Visual C++ Build Tools":
http://landinghub.visualstudio.com/visual-cpp-build-tools

按照上面的错误提示,我们需要先下载和安装Microsoft Visual C++ Build Tools工具,然后才能安装Twisted。
为了安装一个小小的Twisted包,难道就需要安装一个庞大的Microsoft Visual C++ Build  Tools?
答案是否定的,提示上面的错误只是因为pip 自动下载的Twisted安装包有一些缺陷,因此可以先自行下载 Twisted 安装包。
登录 www.lfd.uci.edu/~gohlke/pythonlibs/站点,在该页面中间查找"Twisted"项目,可以看到如图20.1所示的下载链接。
从图20.1所示的链接可以看到,当前Twisted 的最新版是18.7.0, Twisted为2.7、3.4、3.5、 3.6等不同版本的Python 提供了对应的安装包。
由于本书内容主要以Python 3.6为主,因此应该下载 Twisted的 Python 3.6版本,其中文件名带win32 的是32位版本,
而带win amd64 的则是64位版本,此处还需要根据操作系统的位数选择对应的版本。

在下载了合适的 Twisted 安装包后,会得到一个 Twisted-18.7.0-cp36-cp36m-win_amd64.whl 文件(针对64位系统的),
该文件就是Twisted安装包。

运行如下命令来安装Twisted包 。
pip   install Twisted-18.7.0-cp36-cp36m-win_amd64.whl

在安装过程中会自动检查,如有必要,会自动下载并安装 Twisted 所依赖的第三方包,如 zope.interface、Automat、incremental等。
在安装完成后,会提示如下安装成功的信息。
Successfully           installed           Twisted- 18.7.0

在成功安装Twisted包之后,再次执行pip install scrapy命令,即可成功安装Scrapy。 在安装成功后,会显示如下提示信息。
Successfully          installed          Scrapy- 1.5.1

在成功安装Scrapy之后,可以通过 pydoc 来查看Scrapy 的文档。在命令行窗口中输入如下命令。 python   -m   pydoc   -p   8899
运行上面命令之后,打开浏览器查看 http://localhost:8899/页面,可以在 Python
安装目录的 lib\site-packages下看到Scrapy 的文档,如图20.2所示。